{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97ac8e9-6517-4270-995e-52fc3adb5a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import math "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b91c89b-6647-4b7c-b9f8-68ac85bd5ad8",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fcd225-fe35-411f-b911-64b6a9e56ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    block_size: int = 50 # length of the input sequences\n",
    "    vocab_size: int = 1  # time series dimensionality\n",
    "    n_layer: int = 2\n",
    "    n_embd: int = 8\n",
    "    n_head: int = 4\n",
    "\n",
    "class NewGELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n",
    "    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
    "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
    "    explicit implementation here to show that there is nothing too scary here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" an unassuming Transformer block \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = nn.ModuleDict(dict(\n",
    "            c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            c_proj  = nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            act     = NewGELU(),\n",
    "        ))\n",
    "        m = self.mlp\n",
    "        self.mlpf = lambda x: m.c_proj(m.act(m.c_fc(x))) # MLP forward\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlpf(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\" Transformer Language Model, exactly as seen in GPT-2 \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Linear(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
    "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
    "        print(\"number of parameters: %.2fk\" % (n_params/1e3,))\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx.unsqueeze(-1)) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x).squeeze(-1)\n",
    "\n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.mse_loss(logits,targets)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c58d630-fab2-4129-987a-ba83d1e3fad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 2.18k\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(ModelConfig())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a4b2a3-c749-4b79-a87b-de51364ada82",
   "metadata": {},
   "source": [
    "## time series modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c608e9f-f41a-4cbd-8816-4a1281e32b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb263a6-2309-450d-8117-3c862dbaf7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/AEP_hourly.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c19835-792b-43ed-8d00-fb1024eac0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377349be-6b25-449d-9acc-e26ecbe4f8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = df[\"AEP_MW\"].to_numpy(dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea34c2e-3e13-46eb-a6c3-2398ec04c5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_train = ts[:100000]\n",
    "ts_test = ts[100000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53804eca-f319-4b6f-bf62-0bd12dae3fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TS(Dataset):\n",
    "    def __init__(self,ts):\n",
    "        super().__init__()\n",
    "        self.ts = ts\n",
    "    def __len__(self):\n",
    "        return len(self.ts)-50\n",
    "    def __getitem__(self,idx):\n",
    "        x = self.ts[idx:idx+50]\n",
    "        y = self.ts[idx+1:idx+51] \n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e728eac2-9ea5-43cd-8f54-264b8d13dfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 499: train loss 150321120.0\n",
      "batch 999: train loss 182423840.0\n",
      "batch 1499: train loss 164693472.0\n",
      "batch 1999: train loss 136400480.0\n",
      "batch 2499: train loss 125565208.0\n",
      "batch 2999: train loss 104868904.0\n",
      "batch 3499: train loss 133050736.0\n",
      "batch 3999: train loss 113600448.0\n",
      "batch 4499: train loss 123169528.0\n",
      "batch 4999: train loss 116370864.0\n",
      "batch 5499: train loss 94413856.0\n",
      "batch 5999: train loss 106890096.0\n",
      "batch 6499: train loss 113709920.0\n",
      "batch 6999: train loss 88361824.0\n",
      "batch 7499: train loss 73270776.0\n",
      "batch 7999: train loss 72086280.0\n",
      "batch 8499: train loss 84046464.0\n",
      "batch 8999: train loss 83209264.0\n",
      "batch 9499: train loss 70266000.0\n",
      "batch 9999: train loss 59385228.0\n",
      "batch 10499: train loss 71721872.0\n",
      "batch 10999: train loss 50568536.0\n",
      "batch 11499: train loss 50120736.0\n",
      "batch 11999: train loss 36134780.0\n",
      "batch 499: train loss 41485992.0\n",
      "batch 999: train loss 43334872.0\n",
      "batch 1499: train loss 44712500.0\n",
      "batch 1999: train loss 31564812.0\n",
      "batch 2499: train loss 44551392.0\n",
      "batch 2999: train loss 17485534.0\n",
      "batch 3499: train loss 20875600.0\n",
      "batch 3999: train loss 34960972.0\n",
      "batch 4499: train loss 38408052.0\n",
      "batch 4999: train loss 15790129.0\n",
      "batch 5499: train loss 21960344.0\n",
      "batch 5999: train loss 29097768.0\n",
      "batch 6499: train loss 19233710.0\n",
      "batch 6999: train loss 12848772.0\n",
      "batch 7499: train loss 18463666.0\n",
      "batch 7999: train loss 13308224.0\n",
      "batch 8499: train loss 6070457.0\n",
      "batch 8999: train loss 4079113.5\n",
      "batch 9499: train loss 5207956.5\n",
      "batch 9999: train loss 3410301.5\n",
      "batch 10499: train loss 5598388.0\n",
      "batch 10999: train loss 11346445.0\n",
      "batch 11499: train loss 3397325.5\n",
      "batch 11999: train loss 1839463.5\n",
      "batch 499: train loss 2703833.5\n",
      "batch 999: train loss 5017962.0\n",
      "batch 1499: train loss 2663151.25\n",
      "batch 1999: train loss 3352293.0\n",
      "batch 2499: train loss 1014378.1875\n",
      "batch 2999: train loss 1065532.0\n",
      "batch 3499: train loss 1171868.625\n",
      "batch 3999: train loss 908656.75\n",
      "batch 4499: train loss 359758.0\n",
      "batch 4999: train loss 719272.1875\n",
      "batch 5499: train loss 2526773.0\n",
      "batch 5999: train loss 1573494.375\n",
      "batch 6499: train loss 1113524.5\n",
      "batch 6999: train loss 849784.625\n",
      "batch 7499: train loss 816215.4375\n",
      "batch 7999: train loss 487156.75\n",
      "batch 8499: train loss 1384212.375\n",
      "batch 8999: train loss 758859.1875\n",
      "batch 9499: train loss 260959.484375\n",
      "batch 9999: train loss 353257.25\n",
      "batch 10499: train loss 380417.1875\n",
      "batch 10999: train loss 806989.625\n",
      "batch 11499: train loss 907115.625\n",
      "batch 11999: train loss 238145.0625\n",
      "batch 499: train loss 596295.0625\n",
      "batch 999: train loss 376753.03125\n",
      "batch 1499: train loss 684308.8125\n",
      "batch 1999: train loss 228818.234375\n",
      "batch 2499: train loss 488662.75\n",
      "batch 2999: train loss 281271.625\n",
      "batch 3499: train loss 246155.3125\n",
      "batch 3999: train loss 406598.125\n",
      "batch 4499: train loss 331202.4375\n",
      "batch 4999: train loss 222288.59375\n",
      "batch 5499: train loss 250373.4375\n",
      "batch 5999: train loss 891669.25\n",
      "batch 6499: train loss 307180.65625\n",
      "batch 6999: train loss 647138.8125\n",
      "batch 7499: train loss 263498.5\n",
      "batch 7999: train loss 239690.203125\n",
      "batch 8499: train loss 272021.3125\n",
      "batch 8999: train loss 249807.203125\n",
      "batch 9499: train loss 324631.96875\n",
      "batch 9999: train loss 255712.40625\n",
      "batch 10499: train loss 371945.3125\n",
      "batch 10999: train loss 460448.25\n",
      "batch 11499: train loss 261142.359375\n",
      "batch 11999: train loss 248593.578125\n",
      "batch 499: train loss 326393.5625\n",
      "batch 999: train loss 234782.859375\n",
      "batch 1499: train loss 253891.0\n",
      "batch 1999: train loss 488048.96875\n",
      "batch 2499: train loss 256450.65625\n",
      "batch 2999: train loss 207421.78125\n",
      "batch 3499: train loss 423034.96875\n",
      "batch 3999: train loss 591471.875\n",
      "batch 4499: train loss 178551.40625\n",
      "batch 4999: train loss 288630.8125\n",
      "batch 5499: train loss 781940.5\n",
      "batch 5999: train loss 265632.59375\n",
      "batch 6499: train loss 292555.65625\n",
      "batch 6999: train loss 302937.0\n",
      "batch 7499: train loss 321667.09375\n",
      "batch 7999: train loss 357828.15625\n",
      "batch 8499: train loss 222762.6875\n",
      "batch 8999: train loss 227876.21875\n",
      "batch 9499: train loss 253824.234375\n",
      "batch 9999: train loss 259400.984375\n",
      "batch 10499: train loss 254797.078125\n",
      "batch 10999: train loss 205651.5\n",
      "batch 11499: train loss 241931.453125\n",
      "batch 11999: train loss 337198.8125\n"
     ]
    }
   ],
   "source": [
    "data = TS(ts_train)\n",
    "loader = DataLoader(data,batch_size=8, shuffle=True)\n",
    "adam  = torch.optim.AdamW(model.parameters())\n",
    "for _ in range(5):\n",
    "    for i,batch in enumerate(loader):\n",
    "        X,Y = batch\n",
    "        model.zero_grad()\n",
    "        logits, loss = model(X,Y)\n",
    "        loss.backward()\n",
    "        adam.step()\n",
    "        if (i+1)%500==0:\n",
    "            print(f\"batch {i}: train loss {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1a681c-c968-4b82-9a47-856b71e43c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = TS(ts_test)\n",
    "test_loader = DataLoader(test_data, batch_size=8)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    losses = []\n",
    "    for i,batch in enumerate(test_loader):\n",
    "        X,Y = batch\n",
    "        logits, loss = model(X,Y)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    test_loss = torch.tensor(losses).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a525165d-c858-4897-b292-fa05a9bf1ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "448.35325428171035"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss**.5 # RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc696b75-d8d1-4cea-ab69-3af1e985e9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = test_data[0]\n",
    "context = torch.tensor(x[:25]).view(1,-1) #  1,t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0117b35-47d8-4969-90b4-9d217f1e97f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 25])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca016651-117b-4103-a79b-b6c7d1a9eeca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits, loss =model(context)\n",
    "logits[:,[-1]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3093f1d3-0236-4e51-950e-b35cb5b3a924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 26])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((context,logits[:,[-1]]),-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0c8eb6-4537-42f2-8631-1b1cf3b8f52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.randint(len(test_data)-50,(1,))\n",
    "x,y = test_data[idx]\n",
    "context = torch.tensor(x[:25]).view(1,-1) #  1,t\n",
    "ahead = 1\n",
    "for _ in range(ahead):\n",
    "    logits, loss =model(context)\n",
    "    context = torch.cat((context,logits[:,[-1]]),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea82e9a6-9a96-427a-8748-7ec62216f9a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370.40526791610296"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(context.squeeze(0)[25:],torch.tensor(x[25:25+ahead])).item()**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36e60ae-c21b-492e-ba88-2597c0ec5d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
